apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: kfserving-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline_compilation_time: '2022-09-05T06:38:05.821751',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "A pipeline for KFServing.",
      "inputs": [{"default": "apply", "name": "action", "optional": true}, {"default":
      "noops", "name": "model_name", "optional": true}, {"default": "kubeflow-user-example-com",
      "name": "namespace", "optional": true}, {"default": "debug-1", "name": "runtime_version",
      "optional": true}], "name": "KFServing pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3}
spec:
  entrypoint: kfserving-pipeline
  templates:
  - name: kfserving-pipeline
    inputs:
      parameters:
      - {name: action}
      - {name: model_name}
      - {name: namespace}
      - {name: runtime_version}
    dag:
      tasks:
      - name: kubeflow-serve-model-using-kfserving
        template: kubeflow-serve-model-using-kfserving
        arguments:
          parameters:
          - {name: action, value: '{{inputs.parameters.action}}'}
          - {name: model_name, value: '{{inputs.parameters.model_name}}'}
          - {name: namespace, value: '{{inputs.parameters.namespace}}'}
          - {name: runtime_version, value: '{{inputs.parameters.runtime_version}}'}
  - name: kubeflow-serve-model-using-kfserving
    container:
      args:
      - -u
      - kfservingdeployer.py
      - --action
      - '{{inputs.parameters.action}}'
      - --model-name
      - ''
      - --model-uri
      - ''
      - --canary-traffic-percent
      - '100'
      - --namespace
      - ''
      - --framework
      - ''
      - --custom-model-spec
      - '{}'
      - --autoscaling-target
      - '0'
      - --service-account
      - ''
      - --enable-istio-sidecar
      - "True"
      - --output-path
      - /tmp/outputs/InferenceService_Status/data
      - --inferenceservice-yaml
      - |2

          apiVersion: "serving.kubeflow.org/v1beta1"
          kind: "InferenceService"
          metadata:
            name: {{inputs.parameters.model_name}}
            namespace: {{inputs.parameters.namespace}}
            annotations:
            serving.kserve.io/deploymentMode: RawDeployment
            serving.kserve.io/autoscalerClass: hpa
            serving.kserve.io/metric: cpu
            serving.kserve.io/targetUtilizationPercentage: "80"
          spec:
            predictor:
              minReplicas: 1
              maxReplicas: 3
              containers:
              - name: kfserving-container
                image: docker.io/footprintai/iccc-noops:{{inputs.parameters.runtime_version}}
                env:
                - name: DEFAULT_MODEL_NAME
                  value: {{inputs.parameters.model_name}}
                resources:
                  limits:
                    memory: "100Mi"
                    cpu: "100m"
                  requests:
                    memory: "100Mi"
                    cpu: "100m"
      - --watch-timeout
      - '300'
      - --min-replicas
      - '-1'
      - --max-replicas
      - '-1'
      - --request-timeout
      - '60'
      command: [python]
      image: quay.io/aipipeline/kfserving-component:v0.5.1
    inputs:
      parameters:
      - {name: action}
      - {name: model_name}
      - {name: namespace}
      - {name: runtime_version}
    outputs:
      artifacts:
      - {name: kubeflow-serve-model-using-kfserving-InferenceService-Status, path: /tmp/outputs/InferenceService_Status/data}
    metadata:
      labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline-sdk-type: kfp}
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Serve
          Models using Kubeflow KFServing", "implementation": {"container": {"args":
          ["-u", "kfservingdeployer.py", "--action", {"inputValue": "Action"}, "--model-name",
          {"inputValue": "Model Name"}, "--model-uri", {"inputValue": "Model URI"},
          "--canary-traffic-percent", {"inputValue": "Canary Traffic Percent"}, "--namespace",
          {"inputValue": "Namespace"}, "--framework", {"inputValue": "Framework"},
          "--custom-model-spec", {"inputValue": "Custom Model Spec"}, "--autoscaling-target",
          {"inputValue": "Autoscaling Target"}, "--service-account", {"inputValue":
          "Service Account"}, "--enable-istio-sidecar", {"inputValue": "Enable Istio
          Sidecar"}, "--output-path", {"outputPath": "InferenceService Status"}, "--inferenceservice-yaml",
          {"inputValue": "InferenceService YAML"}, "--watch-timeout", {"inputValue":
          "Watch Timeout"}, "--min-replicas", {"inputValue": "Min Replicas"}, "--max-replicas",
          {"inputValue": "Max Replicas"}, "--request-timeout", {"inputValue": "Request
          Timeout"}], "command": ["python"], "image": "quay.io/aipipeline/kfserving-component:v0.5.1"}},
          "inputs": [{"default": "create", "description": "Action to execute on KFServing",
          "name": "Action", "type": "String"}, {"default": "", "description": "Name
          to give to the deployed model", "name": "Model Name", "type": "String"},
          {"default": "", "description": "Path of the S3 or GCS compatible directory
          containing the model.", "name": "Model URI", "type": "String"}, {"default":
          "100", "description": "The traffic split percentage between the candidate
          model and the last ready model", "name": "Canary Traffic Percent", "type":
          "String"}, {"default": "", "description": "Kubernetes namespace where the
          KFServing service is deployed.", "name": "Namespace", "type": "String"},
          {"default": "", "description": "Machine Learning Framework for Model Serving.",
          "name": "Framework", "type": "String"}, {"default": "{}", "description":
          "Custom model runtime container spec in JSON", "name": "Custom Model Spec",
          "type": "String"}, {"default": "0", "description": "Autoscaling Target Number",
          "name": "Autoscaling Target", "type": "String"}, {"default": "", "description":
          "ServiceAccount to use to run the InferenceService pod", "name": "Service
          Account", "type": "String"}, {"default": "True", "description": "Whether
          to enable istio sidecar injection", "name": "Enable Istio Sidecar", "type":
          "Bool"}, {"default": "{}", "description": "Raw InferenceService serialized
          YAML for deployment", "name": "InferenceService YAML", "type": "String"},
          {"default": "300", "description": "Timeout seconds for watching until InferenceService
          becomes ready.", "name": "Watch Timeout", "type": "String"}, {"default":
          "-1", "description": "Minimum number of InferenceService replicas", "name":
          "Min Replicas", "type": "String"}, {"default": "-1", "description": "Maximum
          number of InferenceService replicas", "name": "Max Replicas", "type": "String"},
          {"default": "60", "description": "Specifies the number of seconds to wait
          before timing out a request to the component.", "name": "Request Timeout",
          "type": "String"}], "name": "Kubeflow - Serve Model using KFServing", "outputs":
          [{"description": "Status JSON output of InferenceService", "name": "InferenceService
          Status", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "84f27d18805744db98e4d08804ea3c0e6ca5daa1a3a90dd057b5323f19d9dd2c", "url":
          "https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/kfserving/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"Action": "{{inputs.parameters.action}}",
          "Autoscaling Target": "0", "Canary Traffic Percent": "100", "Custom Model
          Spec": "{}", "Enable Istio Sidecar": "True", "Framework": "", "InferenceService
          YAML": "\n  apiVersion: \"serving.kubeflow.org/v1beta1\"\n  kind: \"InferenceService\"\n  metadata:\n    name:
          {{inputs.parameters.model_name}}\n    namespace: {{inputs.parameters.namespace}}\n    annotations:\n    serving.kserve.io/deploymentMode:
          RawDeployment\n    serving.kserve.io/autoscalerClass: hpa\n    serving.kserve.io/metric:
          cpu\n    serving.kserve.io/targetUtilizationPercentage: \"80\"\n  spec:\n    predictor:\n      minReplicas:
          1\n      maxReplicas: 3\n      containers:\n      - name: kfserving-container\n        image:
          docker.io/footprintai/iccc-noops:{{inputs.parameters.runtime_version}}\n        env:\n        -
          name: DEFAULT_MODEL_NAME\n          value: {{inputs.parameters.model_name}}\n        resources:\n          limits:\n            memory:
          \"100Mi\"\n            cpu: \"100m\"\n          requests:\n            memory:
          \"100Mi\"\n            cpu: \"100m\"\n", "Max Replicas": "-1", "Min Replicas":
          "-1", "Model Name": "", "Model URI": "", "Namespace": "", "Request Timeout":
          "60", "Service Account": "", "Watch Timeout": "300"}'}
  arguments:
    parameters:
    - {name: action, value: apply}
    - {name: model_name, value: noops}
    - {name: namespace, value: kubeflow-user-example-com}
    - {name: runtime_version, value: debug-1}
  serviceAccountName: pipeline-runner

